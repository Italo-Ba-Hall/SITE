"""
Servi√ßo de LLM usando Groq
/-HALL-DEV Backend
"""

import os
import json
import uuid
import hashlib
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
from collections import OrderedDict
import groq
from dotenv import load_dotenv

from schemas import ChatMessage, MessageRole, UserProfile, LLMRequest, LLMResponse

# Carregar vari√°veis de ambiente
load_dotenv()

class LLMCache:
    """Sistema de cache para respostas do LLM"""
    
    def __init__(self, max_size: int = 1000, ttl_hours: int = 24):
        self.cache: OrderedDict[str, Dict] = OrderedDict()
        self.max_size = max_size
        self.ttl = timedelta(hours=ttl_hours)
    
    def _generate_key(self, messages: List[Dict], user_message: str) -> str:
        """Gera chave √∫nica para cache baseada no contexto e mensagem"""
        context_str = json.dumps([msg["content"] for msg in messages], sort_keys=True)
        combined = f"{context_str}:{user_message}"
        return hashlib.md5(combined.encode()).hexdigest()
    
    def get(self, key: str) -> Optional[LLMResponse]:
        """Recupera resposta do cache"""
        if key in self.cache:
            cached = self.cache[key]
            if datetime.now() - cached["timestamp"] < self.ttl:
                # Mover para o final (LRU)
                self.cache.move_to_end(key)
                return cached["response"]
            else:
                del self.cache[key]
        return None
    
    def set(self, key: str, response: LLMResponse):
        """Armazena resposta no cache"""
        if len(self.cache) >= self.max_size:
            # Remove item mais antigo (primeiro da OrderedDict) - O(1)
            self.cache.popitem(last=False)
        
        self.cache[key] = {
            "response": response,
            "timestamp": datetime.now()
        }
        # Mover para o final (LRU)
        self.cache.move_to_end(key)
    
    def clear_expired(self):
        """Remove itens expirados do cache"""
        current_time = datetime.now()
        expired_keys = [
            key for key, value in self.cache.items()
            if current_time - value["timestamp"] > self.ttl
        ]
        for key in expired_keys:
            del self.cache[key]
    
    def get_stats(self) -> Dict:
        """Retorna estat√≠sticas do cache"""
        return {
            "size": len(self.cache),
            "max_size": self.max_size,
            "ttl_hours": self.ttl.total_seconds() / 3600
        }

class LLMService:
    """Servi√ßo para integra√ß√£o com Groq LLM"""
    
    def __init__(self):
        # Verificar se API key est√° definida
        api_key = os.getenv("GROQ_API_KEY")
        if not api_key:
            raise ValueError("GROQ_API_KEY n√£o est√° definida no ambiente")
        
        self.client = groq.Groq(api_key=api_key)
        # Modelo mais robusto: Llama-3-70B vs 8B anterior
        self.model = "llama3-70b-8192"  # Modelo premium gratuito do Groq
        self.max_tokens = 1500  # Aumentado para aproveitar modelo maior
        self.temperature = 0.7
        
        # Cache de respostas
        self.cache = LLMCache(max_size=500, ttl_hours=12)
        
        # Rate limiting
        self.request_count = 0
        self.last_reset = datetime.now()
        self.max_requests_per_minute = 60
        
        # Personalidade do agente
        self.system_prompt = """Voc√™ √© um agente conversacional especializado da /-HALL-DEV, uma empresa de solu√ß√µes tecnol√≥gicas.

PERSONALIDADE:
- EXTREMAMENTE CONCISO: M√°ximo 2-3 frases por resposta
- DIRETO AO PONTO: Sem explica√ß√µes desnecess√°rias
- PERGUNTADOR ESTRAT√âGICO: Foque apenas em fazer perguntas espec√≠ficas
- NATURAL: Conduza a conversa de forma org√¢nica

ESTRAT√âGIA DE ABORDAGEM:
1. PRIMEIRO PERGUNTE: Sempre comece com perguntas espec√≠ficas
2. ENTENDA A DOR: Descubra qual problema o usu√°rio precisa resolver
3. COLETE DADOS NATURALMENTE: Nome e email durante a conversa
4. SUGIRA REUNI√ÉO: Termine propondo agendamento flex√≠vel

PERGUNTAS ESTRAT√âGICAS PARA USAR:
- "Que tipo de processo voc√™ gostaria de melhorar?"
- "Qual √© o maior desafio que voc√™ est√° enfrentando?"
- "Como isso est√° impactando seus resultados?"
- "Que tipo de solu√ß√£o voc√™ imagina que resolveria isso?"
- "Qual seria o impacto ideal para sua empresa?"

SERVI√áOS DA EMPRESA:
- Desenvolvimento de Software
- Business Intelligence (BI)
- Machine Learning
- Automa√ß√£o e RPA
- Intelig√™ncia Artificial

INSTRU√á√ïES DE FORMATA√á√ÉO OBRIGAT√ìRIAS:
IMPORTANTE: SEMPRE use formata√ß√£o visual para tornar suas respostas mais amig√°veis e leg√≠veis:

1. EMOJIS: Use emojis relevantes para tornar o texto mais humano e amig√°vel
   - ‚úÖ Para confirma√ß√µes
   - üí° Para ideias/sugest√µes
   - üîß Para solu√ß√µes t√©cnicas
   - üìä Para dados/KPIs
   - üéØ Para objetivos
   - üëã Para sauda√ß√µes
   - üìß Para contatos
   - ‚ö° Para urg√™ncia/efici√™ncia
   - ü§ñ Para automa√ß√£o/bots
   - üìÖ Para agendamentos
   - üë• Para equipes/pessoas
   - ‚ùì Para perguntas

2. ESTRUTURA VISUAL OBRIGAT√ìRIA:
   - SEMPRE use quebras de linha (\\n) para separar ideias
   - Crie t√≥picos com ‚Ä¢ ou - para listas
   - Use espa√ßamento adequado entre se√ß√µes
   - Destaque informa√ß√µes importantes
   - SEMPRE pule uma linha antes de listas ou t√≥picos

3. EXEMPLO DE FORMATA√á√ÉO CORRETA:
```
üëã Ol√°! Que prazer em conhec√™-lo!

‚ùì Para te ajudar melhor, me conte:

‚Ä¢ Que tipo de processo voc√™ gostaria de melhorar?
‚Ä¢ Qual √© o maior desafio que est√° enfrentando?

üí° Assim posso entender exatamente como posso te ajudar!
```

4. REGRAS OBRIGAT√ìRIAS:
- SEMPRE use \\n para quebras de linha
- SEMPRE pule uma linha antes de listas
- Use emojis com modera√ß√£o (n√£o exagere)
- Mantenha o texto bem estruturado
- Fa√ßa perguntas espec√≠ficas
- Colete dados naturalmente durante a conversa
- SEMPRE formate listas com quebras de linha adequadas
- SEJA EXTREMAMENTE CONCISO: M√°ximo 2-3 frases
- FOCE EM PERGUNTAR: Mais perguntas, menos explica√ß√µes
- NUNCA IGNORE A PRIMEIRA MENSAGEM: Sempre responda ao conte√∫do espec√≠fico

FORMATO DE RESPOSTA:
Responda de forma natural, amig√°vel e bem estruturada. Use emojis e formata√ß√£o visual para tornar a experi√™ncia mais agrad√°vel. SEMPRE aplique quebras de linha adequadas. SEJA EXTREMAMENTE CONCISO E DIRETO. NUNCA IGNORE O CONTE√öDO DA PRIMEIRA MENSAGEM DO USU√ÅRIO."""

    def _check_rate_limit(self) -> bool:
        """Verifica se n√£o excedeu o rate limit"""
        current_time = datetime.now()
        
        # Reset contador a cada minuto
        if current_time - self.last_reset > timedelta(minutes=1):
            self.request_count = 0
            self.last_reset = current_time
        
        if self.request_count >= self.max_requests_per_minute:
            return False
        
        self.request_count += 1
        return True

    def _build_conversation_context(self, messages: List[ChatMessage]) -> List[Dict[str, str]]:
        """Constr√≥i o contexto da conversa para o LLM"""
        context = [{"role": "system", "content": self.system_prompt}]
        
        for message in messages:
            context.append({
                "role": message.role.value,
                "content": message.content
            })
        
        return context

    def _extract_user_profile(self, message: str) -> Optional[Dict[str, str]]:
        """Extrai informa√ß√µes do usu√°rio da mensagem"""
        # L√≥gica simples de extra√ß√£o - pode ser melhorada
        profile = {}
        
        # Extrair nome (padr√µes comuns)
        import re
        name_patterns = [
            r"meu nome √© (\w+)",
            r"eu sou (\w+)",
            r"chamo-me (\w+)",
            r"sou (\w+)"
        ]
        
        for pattern in name_patterns:
            match = re.search(pattern, message.lower())
            if match:
                profile["name"] = match.group(1).title()
                break
        
        # Extrair email
        email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        email_match = re.search(email_pattern, message)
        if email_match:
            profile["email"] = email_match.group(0)
        
        return profile if profile else None

    def _detect_intent(self, message: str) -> Optional[str]:
        """Detecta a inten√ß√£o da mensagem do usu√°rio"""
        message_lower = message.lower()
        
        intents = {
            "greeting": ["ol√°", "oi", "bom dia", "boa tarde", "boa noite"],
            "problem_description": ["problema", "dificuldade", "dor", "preciso", "quero"],
            "service_inquiry": ["servi√ßo", "solu√ß√£o", "desenvolvimento", "software"],
            "contact_info": ["contato", "email", "telefone", "whatsapp"],
            "pricing": ["pre√ßo", "valor", "custo", "or√ßamento"],
            "technical": ["tecnologia", "programa√ß√£o", "c√≥digo", "sistema"]
        }
        
        for intent, keywords in intents.items():
            if any(keyword in message_lower for keyword in keywords):
                return intent
        
        return None

    def _optimize_prompt_size(self, context: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """Otimiza o tamanho do prompt para reduzir tokens"""
        if len(context) <= 10:  # Se j√° √© pequeno, retorna como est√°
            return context
        
        # Manter system prompt e √∫ltimas 8 mensagens
        optimized = [context[0]]  # System prompt
        optimized.extend(context[-8:])  # √öltimas 8 mensagens
        
        return optimized

    async def generate_response(self, request: LLMRequest) -> LLMResponse:
        """Gera resposta usando Groq LLM com cache e otimiza√ß√µes"""
        try:
            # Validar entrada
            if not request.message or not request.message.strip():
                return LLMResponse(
                    message="Por favor, digite uma mensagem para que eu possa te ajudar.",
                    session_id=request.session_id,
                    confidence=0.0,
                    metadata={"error": "empty_message"}
                )
            
            # Verificar rate limit
            if not self._check_rate_limit():
                return LLMResponse(
                    message="Desculpe, estamos com muitas solicita√ß√µes no momento. Tente novamente em alguns instantes.",
                    session_id=request.session_id,
                    confidence=0.0,
                    metadata={"error": "rate_limit_exceeded"}
                )
            
            # Construir contexto da conversa
            conversation_context = self._build_conversation_context(request.context.get("messages", []))
            
            # Otimizar tamanho do prompt
            optimized_context = self._optimize_prompt_size(conversation_context)
            
            # Adicionar mensagem atual do usu√°rio
            optimized_context.append({
                "role": "user",
                "content": request.message
            })
            
            # Verificar cache
            cache_key = self.cache._generate_key(optimized_context[:-1], request.message)
            cached_response = self.cache.get(cache_key)
            
            if cached_response:
                cached_response.session_id = request.session_id
                cached_response.metadata = {
                    **cached_response.metadata,
                    "cached": True,
                    "cache_hit": True
                }
                return cached_response
            
            # Chamar Groq API
            completion = self.client.chat.completions.create(
                model=self.model,
                messages=optimized_context,
                max_tokens=self.max_tokens,
                temperature=self.temperature,
                stream=False
            )
            
            # Extrair resposta
            response_content = completion.choices[0].message.content
            
            # Extrair informa√ß√µes do usu√°rio
            user_profile = self._extract_user_profile(request.message)
            
            # Detectar inten√ß√£o
            intent = self._detect_intent(request.message)
            
            # Calcular confian√ßa baseada na resposta
            confidence = 0.8  # Base inicial, pode ser melhorada
            
            response = LLMResponse(
                message=response_content,
                session_id=request.session_id,
                user_profile_extracted=user_profile,
                intent_detected=intent,
                confidence=confidence,
                metadata={
                    "model": self.model,
                    "tokens_used": completion.usage.total_tokens if hasattr(completion, 'usage') else None,
                    "timestamp": datetime.now().isoformat(),
                    "cached": False,
                    "cache_hit": False
                }
            )
            
            # Armazenar no cache
            self.cache.set(cache_key, response)
            
            return response
            
        except Exception as e:
            # Fallback em caso de erro
            fallback_response = "Desculpe, estou com dificuldades t√©cnicas no momento. Pode tentar novamente em alguns instantes?"
            
            return LLMResponse(
                message=fallback_response,
                session_id=request.session_id,
                confidence=0.0,
                metadata={"error": str(e), "fallback": True}
            )

    def create_welcome_message(self) -> str:
        """Cria mensagem de boas-vindas personalizada"""
        return """üëã Ol√°! Que prazer em conhec√™-lo!

‚ùì Para te ajudar melhor, me conte:

‚Ä¢ Que tipo de processo voc√™ gostaria de melhorar?
‚Ä¢ Qual √© o maior desafio que est√° enfrentando?

üí° Assim posso entender exatamente como posso te ajudar!"""

    def create_session_id(self) -> str:
        """Cria ID √∫nico para sess√£o"""
        return f"session_{uuid.uuid4().hex[:8]}"
    
    def get_cache_stats(self) -> Dict:
        """Retorna estat√≠sticas do cache"""
        return self.cache.get_stats()
    
    def get_stats(self) -> Dict:
        """Retorna estat√≠sticas gerais do LLM Service"""
        return {
            "model": self.model,
            "max_tokens": self.max_tokens,
            "temperature": self.temperature,
            "cache_stats": self.cache.get_stats(),
            "request_count": self.request_count,
            "max_requests_per_minute": self.max_requests_per_minute
        }
    
    def clear_cache(self):
        """Limpa o cache"""
        self.cache.clear()
    
    def cleanup_cache(self):
        """Remove itens expirados do cache"""
        self.cache.clear_expired()
    
    def auto_cleanup_cache(self):
        """Limpeza autom√°tica do cache - chamar periodicamente"""
        if len(self.cache.cache) > self.cache.max_size * 0.8:  # Se 80% cheio
            self.cache.clear_expired()
            # Se ainda estiver cheio, remover 20% dos itens mais antigos
            if len(self.cache.cache) > self.cache.max_size * 0.8:
                items_to_remove = int(len(self.cache.cache) * 0.2)
                for _ in range(items_to_remove):
                    if self.cache.cache:
                        self.cache.cache.popitem(last=False)

# Inst√¢ncia global do servi√ßo
llm_service = LLMService() 