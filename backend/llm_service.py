"""
Servi√ßo de LLM usando Google Gemini
/-HALL-DEV Backend
"""

import hashlib
import json
import logging
import os
import uuid
from collections import OrderedDict
from datetime import datetime, timedelta
from typing import Any

import google.generativeai as genai
from dotenv import load_dotenv

from schemas import (
    ChatMessage,
    LLMRequest,
    LLMResponse,
    MessageRole,
    Phase,
)

# Configurar logger
logger = logging.getLogger(__name__)

# Carregar vari√°veis de ambiente
load_dotenv()


class LLMCache:
    """Sistema de cache para respostas do LLM"""

    def __init__(self, max_size: int = 1000, ttl_hours: int = 24):
        self.cache: OrderedDict[str, dict] = OrderedDict()
        self.max_size = max_size
        self.ttl = timedelta(hours=ttl_hours)

    def _generate_key(self, messages: list[dict], user_message: str) -> str:
        """Gera chave √∫nica para cache baseada no contexto e mensagem"""
        context_str = json.dumps([msg["content"] for msg in messages], sort_keys=True)
        combined = f"{context_str}:{user_message}"
        return hashlib.sha256(combined.encode()).hexdigest()

    def get(self, key: str) -> LLMResponse | None:
        """Recupera resposta do cache"""
        if key in self.cache:
            cached = self.cache[key]
            if datetime.now() - cached["timestamp"] < self.ttl:
                # Mover para o final (LRU)
                self.cache.move_to_end(key)
                return cached["response"]
            else:
                del self.cache[key]
        return None

    def set(self, key: str, response: LLMResponse):
        """Armazena resposta no cache"""
        if len(self.cache) >= self.max_size:
            # Remove item mais antigo (primeiro da OrderedDict) - O(1)
            self.cache.popitem(last=False)

        self.cache[key] = {"response": response, "timestamp": datetime.now()}
        # Mover para o final (LRU)
        self.cache.move_to_end(key)

    def clear_expired(self):
        """Remove itens expirados do cache"""
        current_time = datetime.now()
        expired_keys = [
            key
            for key, value in self.cache.items()
            if current_time - value["timestamp"] > self.ttl
        ]
        for key in expired_keys:
            del self.cache[key]

    def get_stats(self) -> dict:
        """Retorna estat√≠sticas do cache"""
        return {
            "size": len(self.cache),
            "max_size": self.max_size,
            "ttl_hours": self.ttl.total_seconds() / 3600,
        }


class LLMService:
    """Servi√ßo para integra√ß√£o com Google Gemini LLM"""

    def __init__(self):
        # Verificar se API key est√° definida
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("GEMINI_API_KEY n√£o est√° definida no ambiente")

        genai.configure(api_key=api_key)
        
        self.model_name = "gemini-2.5-flash"
        self.max_tokens = 1500
        self.temperature = 0.25

        # Cache de respostas
        self.cache = LLMCache(max_size=500, ttl_hours=12)

        # Rate limiting
        self.request_count = 0
        self.last_reset = datetime.now()
        self.max_requests_per_minute = 60

        # Personalidade do agente
        self.system_prompt = """Voc√™ √© um agente conversacional especializado da /-HALL-DEV.

PERSONALIDADE:
- EXTREMAMENTE CONCISO (2-3 frases)
- DIRETO AO PONTO
- NATURAL E AMIG√ÅVEL

FLUXO OBRIGAT√ìRIO (REGRA DE 2 TURNOS):
1) Discovery: responda √† primeira mensagem do usu√°rio com no m√°ximo 2 perguntas espec√≠ficas e pertinentes ao tema.
2) Lead Capture: ap√≥s a primeira resposta do usu√°rio, SE nome e email ainda n√£o foram coletados, pe√ßa os DOIS em UMA √öNICA frase, de forma simples e clara.
3) Scheduling: assim que nome e email forem coletados, proponha AGENDAMENTO (apresente op√ß√µes de hor√°rio ou pe√ßa disponibilidade) e ofere√ßa UMA ESCOLHA: receber ‚Äúexplica√ß√µes t√©cnicas r√°pidas‚Äù antes ou ‚Äúir direto para o agendamento‚Äù.
4) Se o usu√°rio disser ‚Äún√£o sei‚Äù ou pedir orienta√ß√£o, reduza perguntas, colete nome/email e avance para o agendamento.

SERVI√áOS:
- Desenvolvimento de Software, BI, Machine Learning, Automa√ß√£o/RPA, IA

FORMATA√á√ÉO VISUAL OBRIGAT√ìRIA:
- Use quebras de linha (\\n) e listas com ‚Ä¢
- Emojis pontuais: üëã ‚ùì üí° üìß üìÖ üíª üéØ

REGRAS GERAIS:
- Sempre responda ao conte√∫do espec√≠fico do usu√°rio
- Mantenha 2-3 frases; sem par√°grafos longos
- No m√°ximo 2 perguntas por resposta
- Priorize avan√ßar o fluxo (coleta e agendamento)
"""

    def _check_rate_limit(self) -> bool:
        """Verifica se n√£o excedeu o rate limit"""
        current_time = datetime.now()

        # Reset contador a cada minuto
        if current_time - self.last_reset > timedelta(minutes=1):
            self.request_count = 0
            self.last_reset = current_time

        if self.request_count >= self.max_requests_per_minute:
            return False

        self.request_count += 1
        return True

    def _build_conversation_context(
        self, messages: list[ChatMessage]
    ) -> list[dict[str, str]]:
        """Constr√≥i o contexto da conversa para o LLM"""
        context = [{"role": "system", "content": self.system_prompt}]

        for message in messages:
            context.append({"role": message.role.value, "content": message.content})

        return context

    def _get_phase_from_context(self, ctx: dict[str, Any] | None) -> str | None:
        if not ctx:
            return None
        phase_val = ctx.get("phase")
        if isinstance(phase_val, Phase):
            return phase_val.value
        if isinstance(phase_val, str):
            return phase_val.lower()
        return None

    def _compose_policy_instructions(
        self,
        ctx: dict[str, Any] | None,
        user_message: str,
    ) -> str:
        """Gera instru√ß√µes de pol√≠tica contextuais para garantir coleta e agendamento."""
        policy_parts: list[str] = []

        # Detectar incerteza para encurtar e conduzir
        user_lower = user_message.lower()
        uncertainty = any(
            k in user_lower
            for k in [
                "n√£o sei",
                "nao sei",
                "preciso de orienta",
                "n√£o entendo",
                "nao entendo",
            ]
        )

        phase = self._get_phase_from_context(ctx)
        profile = (ctx or {}).get("user_profile") or {}
        has_name = bool(profile.get("name"))
        has_email = bool(profile.get("email"))

        # Contar mensagens do usu√°rio no contexto para gatilho de captura
        user_count = 0
        for m in (ctx or {}).get("messages", []) or []:
            try:
                if getattr(m, "role", None) == MessageRole.USER or (
                    isinstance(m, dict)
                    and (m.get("role") == "user" or m.get("role") == MessageRole.USER)
                ):
                    user_count += 1
            except Exception as e:
                logger.warning(f"Erro ao processar mensagem do contexto: {e}")

        # Regra: ap√≥s a 1¬™ mensagem do usu√°rio, pedir nome e email se faltarem
        if user_count >= 1 and (not has_name or not has_email):
            if uncertainty:
                policy_parts.append(
                    "Usu√°rio incerto: reduza perguntas. Pe√ßa NOME e EMAIL agora em UMA frase curta, ent√£o conduza para agendamento."
                )
            else:
                policy_parts.append(
                    "Se NOME/EMAIL faltarem, pe√ßa ambos AGORA em UMA frase curta. No m√°ximo 1 pergunta adicional."
                )

        # Se j√° coletou nome e email, avan√ßar para agendamento com a bifurca√ß√£o
        if has_name and has_email:
            policy_parts.append(
                "Proponha AGENDAMENTO imediatamente e ofere√ßa escolha: 'explica√ß√µes t√©cnicas r√°pidas' OU 'agendar agora'."
            )

        # Fase espec√≠fica pode refor√ßar comportamento
        if phase == "lead_capture" and (not has_name or not has_email):
            policy_parts.append(
                "Estamos em LEAD_CAPTURE: priorize coletar NOME e EMAIL nesta resposta."
            )
        if phase == "scheduling" and has_name and has_email:
            policy_parts.append(
                "Estamos em SCHEDULING: foque em confirmar data/hor√°rio de reuni√£o."
            )

        # Sempre limitar perguntas
        policy_parts.append("No m√°ximo 2 perguntas na resposta.")

        return " ".join(policy_parts).strip()

    def _extract_user_profile(self, message: str) -> dict[str, str] | None:
        """Extrai informa√ß√µes do usu√°rio da mensagem"""
        # L√≥gica simples de extra√ß√£o - pode ser melhorada
        profile = {}

        # Extrair nome (padr√µes comuns)
        import re

        name_patterns = [
            r"meu nome √©\s+([\w√Ä-√ñ√ò-√∂√∏-√ø\s]{2,})",
            r"eu sou\s+([\w√Ä-√ñ√ò-√∂√∏-√ø\s]{2,})",
            r"chamo-me\s+([\w√Ä-√ñ√ò-√∂√∏-√ø\s]{2,})",
            r"sou\s+([\w√Ä-√ñ√ò-√∂√∏-√ø\s]{2,})",
        ]

        for pattern in name_patterns:
            match = re.search(pattern, message.lower())
            if match:
                profile["name"] = match.group(1).strip().title()
                break

        # Extrair email
        email_pattern = r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b"
        email_match = re.search(email_pattern, message)
        if email_match:
            profile["email"] = email_match.group(0)

        return profile if profile else None

    def _detect_intent(self, message: str) -> str | None:
        """Detecta a inten√ß√£o da mensagem do usu√°rio"""
        message_lower = message.lower()

        intents = {
            "greeting": ["ol√°", "oi", "bom dia", "boa tarde", "boa noite"],
            "mentoring": ["mentoria", "mentor"],
            "learning": ["aprender", "estudar", "curso", "treinamento", "forma√ß√£o"],
            "programming": [
                "programar",
                "programa√ß√£o",
                "c√≥digo",
                "desenvolver",
                "coding",
            ],
            "self_learning": [
                "sozinho",
                "autodidata",
                "independente",
                "por conta pr√≥pria",
            ],
            "help_request": ["ajudar", "ajuda", "suporte", "assist√™ncia"],
            "problem_description": [
                "problema",
                "dificuldade",
                "dor",
                "preciso",
                "quero",
            ],
            "service_inquiry": ["servi√ßo", "solu√ß√£o", "desenvolvimento", "software"],
            "contact_info": ["contato", "email", "telefone", "whatsapp"],
            "pricing": ["pre√ßo", "valor", "custo", "or√ßamento"],
            "technical": ["tecnologia", "programa√ß√£o", "c√≥digo", "sistema"],
        }

        for intent, keywords in intents.items():
            if any(keyword in message_lower for keyword in keywords):
                return intent

        return None

    def _get_contextual_prompt(self, message: str, detected_intent: str | None) -> str:
        """Gera prompt contextual baseado na inten√ß√£o detectada"""

        # Contexto espec√≠fico para mentoria e aprendizado
        if detected_intent in ["mentoring", "learning", "programming", "self_learning"]:
            return """Voc√™ √© um agente da /-HALL-DEV para mentoria/treinamento em programa√ß√£o.

PERSONALIDADE:
- EXTREMAMENTE CONCISO (2-3 frases)
- DIRETO AO PONTO
- NATURAL

SERVI√áOS DE MENTORIA:
- Mentoria Individual em Programa√ß√£o
- Treinamentos Corporativos
- Cursos Personalizados
- Acompanhamento de Projetos
- Consultoria T√©cnica

FLUXO OBRIGAT√ìRIO (2 TURNOS):
1) Discovery: no m√°ximo 2 perguntas espec√≠ficas;
2) Lead Capture: ap√≥s a 1¬™ resposta do usu√°rio, pe√ßa nome e email em UMA frase;
3) Scheduling: ap√≥s coletar nome e email, proponha agendamento e ofere√ßa op√ß√£o: ‚Äúexplica√ß√µes t√©cnicas r√°pidas‚Äù ou ‚Äúmarcar agora‚Äù.

PERGUNTAS ESTRAT√âGICAS PARA MENTORIA:
- "Que linguagem de programa√ß√£o voc√™ quer aprender?"
- "Voc√™ j√° tem alguma experi√™ncia com programa√ß√£o?"
- "Qual √© seu objetivo principal com o aprendizado?"
- "Que tipo de projeto voc√™ gostaria de desenvolver?"
- "Qual seria sua disponibilidade para as sess√µes?"

INSTRU√á√ïES DE FORMATA√á√ÉO OBRIGAT√ìRIAS:
IMPORTANTE: SEMPRE use formata√ß√£o visual para tornar suas respostas mais amig√°veis e leg√≠veis:

1. EMOJIS: Use emojis relevantes para tornar o texto mais humano e amig√°vel
   - ‚úÖ Para confirma√ß√µes
   - üí° Para ideias/sugest√µes
   - üîß Para solu√ß√µes t√©cnicas
   - üìä Para dados/KPIs
   - üéØ Para objetivos
   - üëã Para sauda√ß√µes
   - üìß Para contatos
   - ‚ö° Para urg√™ncia/efici√™ncia
   - ü§ñ Para automa√ß√£o/bots
   - üìÖ Para agendamentos
   - üë• Para equipes/pessoas
   - ‚ùì Para perguntas
   - üéì Para educa√ß√£o/mentoria
   - üíª Para programa√ß√£o/tecnologia

2. ESTRUTURA VISUAL OBRIGAT√ìRIA:
   - SEMPRE use quebras de linha (\\n) para separar ideias
   - Crie t√≥picos com ‚Ä¢ ou - para listas
   - Use espa√ßamento adequado entre se√ß√µes
   - Destaque informa√ß√µes importantes
   - SEMPRE pule uma linha antes de listas ou t√≥picos

3. EXEMPLO DE FORMATA√á√ÉO CORRETA:
```
üéì Que √≥timo! Vamos te ajudar a aprender programa√ß√£o!

‚ùì Para personalizar sua mentoria, me conte:

‚Ä¢ Que linguagem de programa√ß√£o voc√™ quer aprender?
‚Ä¢ Voc√™ j√° tem alguma experi√™ncia com c√≥digo?

üíª Assim posso criar um plano de estudos perfeito para voc√™!
```

4. REGRAS OBRIGAT√ìRIAS:
- SEMPRE use \\n para quebras de linha
- SEMPRE pule uma linha antes de listas
- Use emojis com modera√ß√£o (n√£o exagere)
- Mantenha o texto bem estruturado
- Fa√ßa no m√°ximo 2 perguntas
- Pe√ßa nome e email ap√≥s a 1¬™ resposta do usu√°rio (se ainda n√£o coletados)
- SEMPRE formate listas com quebras de linha adequadas
- SEJA EXTREMAMENTE CONCISO: M√°ximo 2-3 frases
- NUNCA IGNORE A PRIMEIRA MENSAGEM: Responda ao conte√∫do espec√≠fico

FORMATO DE RESPOSTA:
Responda de forma natural e estruturada, com 2-3 frases. Sempre aplique quebras de linha e ofere√ßa a decis√£o: ‚Äúexplica r√°pido‚Äù vs ‚Äúagendar‚Äù."""

        # Contexto espec√≠fico para solicita√ß√µes de ajuda
        elif detected_intent == "help_request":
            return """Voc√™ √© um agente da /-HALL-DEV para ajuda e suporte t√©cnico.

PERSONALIDADE:
- EXTREMAMENTE CONCISO (2-3 frases)
- DIRETO AO PONTO
- NATURAL

SERVI√áOS DE AJUDA:
- Suporte T√©cnico
- Consultoria Especializada
- Resolu√ß√£o de Problemas
- Implementa√ß√£o de Solu√ß√µes
- Treinamento e Capacita√ß√£o

FLUXO OBRIGAT√ìRIO (2 TURNOS):
1) Discovery: no m√°ximo 2 perguntas sobre o problema e impacto;
2) Lead Capture: ap√≥s a 1¬™ resposta do usu√°rio, pe√ßa nome e email em UMA frase;
3) Scheduling: ap√≥s coletar, proponha agendamento e ofere√ßa op√ß√£o: ‚Äúexplica√ß√µes t√©cnicas r√°pidas‚Äù ou ‚Äúmarcar agora‚Äù.

PERGUNTAS ESTRAT√âGICAS PARA AJUDA:
- "Que tipo de problema voc√™ est√° enfrentando?"
- "Qual √© o impacto disso no seu trabalho?"
- "Voc√™ j√° tentou alguma solu√ß√£o?"
- "Qual seria o prazo ideal para resolver?"
- "Que tipo de suporte voc√™ imagina que resolveria?"

INSTRU√á√ïES DE FORMATA√á√ÉO OBRIGAT√ìRIAS:
IMPORTANTE: SEMPRE use formata√ß√£o visual para tornar suas respostas mais amig√°veis e leg√≠veis:

1. EMOJIS: Use emojis relevantes para tornar o texto mais humano e amig√°vel
   - ‚úÖ Para confirma√ß√µes
   - üí° Para ideias/sugest√µes
   - üîß Para solu√ß√µes t√©cnicas
   - üìä Para dados/KPIs
   - üéØ Para objetivos
   - üëã Para sauda√ß√µes
   - üìß Para contatos
   - ‚ö° Para urg√™ncia/efici√™ncia
   - ü§ñ Para automa√ß√£o/bots
   - üìÖ Para agendamentos
   - üë• Para equipes/pessoas
   - ‚ùì Para perguntas
   - üÜò Para ajuda/suporte
   - ‚ö†Ô∏è Para problemas/alertas

2. ESTRUTURA VISUAL OBRIGAT√ìRIA:
   - SEMPRE use quebras de linha (\\n) para separar ideias
   - Crie t√≥picos com ‚Ä¢ ou - para listas
   - Use espa√ßamento adequado entre se√ß√µes
   - Destaque informa√ß√µes importantes
   - SEMPRE pule uma linha antes de listas ou t√≥picos

3. EXEMPLO DE FORMATA√á√ÉO CORRETA:
```
üÜò Entendo! Vamos te ajudar a resolver isso!

‚ùì Para te dar o melhor suporte, me conte:

‚Ä¢ Que tipo de problema voc√™ est√° enfrentando?
‚Ä¢ Qual √© o impacto disso no seu trabalho?

üîß Assim posso conectar voc√™ com a solu√ß√£o ideal!
```

4. REGRAS OBRIGAT√ìRIAS:
- SEMPRE use \\n para quebras de linha
- SEMPRE pule uma linha antes de listas
- Use emojis com modera√ß√£o (n√£o exagere)
- Mantenha o texto bem estruturado
- Fa√ßa no m√°ximo 2 perguntas
- Pe√ßa nome e email ap√≥s a 1¬™ resposta do usu√°rio (se ainda n√£o coletados)
- SEMPRE formate listas com quebras de linha adequadas
- SEJA EXTREMAMENTE CONCISO: M√°ximo 2-3 frases
- NUNCA IGNORE A PRIMEIRA MENSAGEM: Responda ao conte√∫do espec√≠fico

FORMATO DE RESPOSTA:
Responda de forma natural e estruturada, com 2-3 frases. Sempre aplique quebras de linha e ofere√ßa a decis√£o: ‚Äúexplica r√°pido‚Äù vs ‚Äúagendar‚Äù."""

        # Contexto padr√£o para outras inten√ß√µes
        else:
            return self.system_prompt

    def _optimize_prompt_size(
        self, context: list[dict[str, str]]
    ) -> list[dict[str, str]]:
        """Otimiza o tamanho do prompt para reduzir tokens"""
        if len(context) <= 10:  # Se j√° √© pequeno, retorna como est√°
            return context

        # Manter system prompt e √∫ltimas 8 mensagens
        optimized = [context[0]]  # System prompt
        optimized.extend(context[-8:])  # √öltimas 8 mensagens

        return optimized

    async def generate_response(self, request: LLMRequest) -> LLMResponse:
        """Gera resposta usando Groq LLM com cache e otimiza√ß√µes"""
        try:
            # Validar entrada
            if not request.message or not request.message.strip():
                return LLMResponse(
                    message="Por favor, digite uma mensagem para que eu possa te ajudar.",
                    session_id=request.session_id,
                    confidence=0.0,
                    metadata={"error": "empty_message"},
                )

            # Verificar rate limit
            if not self._check_rate_limit():
                return LLMResponse(
                    message="Desculpe, estamos com muitas solicita√ß√µes no momento. Tente novamente em alguns instantes.",
                    session_id=request.session_id,
                    confidence=0.0,
                    metadata={"error": "rate_limit_exceeded"},
                )

            # Construir contexto da conversa
            messages = (request.context or {}).get("messages", [])
            conversation_context = self._build_conversation_context(messages)

            # Otimizar tamanho do prompt
            optimized_context = self._optimize_prompt_size(conversation_context)

            # Adicionar mensagem atual do usu√°rio
            optimized_context.append({"role": "user", "content": request.message})

            # Verificar cache
            cache_key = self.cache._generate_key(
                optimized_context[:-1], request.message
            )
            cached_response = self.cache.get(cache_key)

            if cached_response:
                cached_response.session_id = request.session_id
                cached_response.metadata = {
                    **cached_response.metadata,
                    "cached": True,
                    "cache_hit": True,
                }
                return cached_response

            # Detectar inten√ß√£o
            detected_intent = self._detect_intent(request.message)

            # Gerar prompt contextual
            contextual_prompt = self._get_contextual_prompt(
                request.message, detected_intent
            )

            # Injetar pol√≠tica determin√≠stica de captura/agendamento
            policy_instructions = self._compose_policy_instructions(
                request.context, request.message
            )
            if policy_instructions:
                contextual_prompt = f"{contextual_prompt}\n\nPOL√çTICA ATUAL (OBRIGAT√ìRIA): {policy_instructions}"

            # Criar modelo com system instruction contextual
            model = genai.GenerativeModel(
                self.model_name,
                system_instruction=contextual_prompt
            )

            # Converter hist√≥rico para formato Gemini
            history_parts = []
            for msg in optimized_context[1:]:  # Pular system prompt (j√° est√° no model)
                role = "user" if msg["role"] == "user" else "model"
                history_parts.append({
                    "role": role,
                    "parts": [msg["content"]]
                })

            # Criar chat com hist√≥rico
            chat = model.start_chat(history=history_parts)

            # Enviar mensagem atual
            gemini_response = chat.send_message(
                request.message,
                generation_config=genai.types.GenerationConfig(
                    max_output_tokens=self.max_tokens,
                    temperature=self.temperature,
                )
            )

            # Extrair resposta
            response_content = gemini_response.text

            # Extrair informa√ß√µes do usu√°rio
            user_profile = self._extract_user_profile(request.message)

            # Usar a inten√ß√£o j√° detectada
            intent = detected_intent

            # Calcular confian√ßa baseada na resposta
            confidence = 0.8  # Base inicial, pode ser melhorada

            # Extrair tokens usados
            tokens_used = None
            if hasattr(gemini_response, 'usage_metadata'):
                tokens_used = gemini_response.usage_metadata.total_token_count

            response = LLMResponse(
                message=response_content,
                session_id=request.session_id,
                user_profile_extracted=user_profile,
                intent_detected=intent,
                confidence=confidence,
                metadata={
                    "model": self.model_name,
                    "tokens_used": tokens_used,
                    "timestamp": datetime.now().isoformat(),
                    "cached": False,
                    "cache_hit": False,
                },
            )

            # Armazenar no cache
            self.cache.set(cache_key, response)

            return response

        except Exception as e:
            # Fallback em caso de erro
            fallback_response = "Desculpe, estou com dificuldades t√©cnicas no momento. Pode tentar novamente em alguns instantes?"

            return LLMResponse(
                message=fallback_response,
                session_id=request.session_id,
                confidence=0.0,
                metadata={"error": str(e), "fallback": True},
            )

    def create_welcome_message(self) -> str:
        """Cria mensagem de boas-vindas personalizada"""
        return """üëã Ol√°! Que prazer em conhec√™-lo!

‚ùì Para te ajudar melhor, me conte:

‚Ä¢ Que tipo de processo voc√™ gostaria de melhorar?
‚Ä¢ Qual √© o maior desafio que est√° enfrentando?

üí° Assim posso entender exatamente como posso te ajudar!"""

    def create_session_id(self) -> str:
        """Cria ID √∫nico para sess√£o"""
        return f"session_{uuid.uuid4().hex[:8]}"

    def get_cache_stats(self) -> dict:
        """Retorna estat√≠sticas do cache"""
        return self.cache.get_stats()

    def get_stats(self) -> dict:
        """Retorna estat√≠sticas gerais do LLM Service"""
        return {
            "model": self.model_name,
            "max_tokens": self.max_tokens,
            "temperature": self.temperature,
            "cache_stats": self.cache.get_stats(),
            "request_count": self.request_count,
            "max_requests_per_minute": self.max_requests_per_minute,
        }

    def clear_cache(self):
        """Limpa o cache"""
        self.cache.clear()

    def cleanup_cache(self):
        """Remove itens expirados do cache"""
        self.cache.clear_expired()

    def auto_cleanup_cache(self):
        """Limpeza autom√°tica do cache - chamar periodicamente"""
        if len(self.cache.cache) > self.cache.max_size * 0.8:  # Se 80% cheio
            self.cache.clear_expired()
            # Se ainda estiver cheio, remover 20% dos itens mais antigos
            if len(self.cache.cache) > self.cache.max_size * 0.8:
                items_to_remove = int(len(self.cache.cache) * 0.2)
                for _ in range(items_to_remove):
                    if self.cache.cache:
                        self.cache.cache.popitem(last=False)


# Inst√¢ncia global do servi√ßo
llm_service = LLMService()
